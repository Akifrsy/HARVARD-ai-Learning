Supervised Learning: Given a data set of input-output pairs, learn a function to map inputs to outputs.

Classification: Supervised learning task of learning a function mapping an input point to a discrete category.



Date      | Humidity | Pressure |  Rain   |
January 1 |   93%    |   999.7  |  Rain   |
January 2 |   49%    |  1015.5  | No Rain |
January 3 |   79%    |  1031.1  | No Rain |
January 4 |   65%    |   984.9  |  Rain   |
January 5 |   90%    |   975.2  |  Rain   |

f(humidity, pressure)
    f(93, 999.7) = Rain
    f(49, 1015.5) = No Rain
    f(79, 1031.1) = No Rain
h(humidity, pressure)

Nearest-Neighbor Classification: Algorithm that, given an input, chooses the class of the nearest data point to that input.

k-Nearest-Neighbor Classification: Algorithm that, given an input , chooses the most common class out of the k nearest data points to that input.

x1 = Humidity
x2 = Pressure

h(x1, x2) = Rain if w0 + w1x1 + w2x2 > 0 / h(x1, x2) = 1 if w0 + w1x1 + w2x2 > 0 / 0 otherwise

Weight Vector w: (w0, w1, w2)
Input Vector x: (1, x1, x2)
w . x: w0 + w1x1 + w2x2

hw(x) = 1 if w.x > 0 / 0 otherwise

Perceptron Learning Rule: Given data point (x, y), update each weight according to: wi = wi + a(y - hw(x)) x Xi

Maximum Margin Separator: Boundary that maximizes the distance between any of the data points.

Regrassion: Supervised learning task of learning a function mapping an input point to a continuous value.

f(advertising)
    f(1200) = 5800
    f(2800) = 13400
    f(1800) = 8400
h(advertising)

Loss Function: Function that express how poorly our hypothesis performs.

0-1 Loss Function: 
    L(actual, predicted) =
        0 if actual = predicted,
        1 otherwise

L1 Loss Function: L(actual, predicted) = | actual - predicted |

Overfitting: A model that fits too closely to a particular data set and therefore may fail to generalize to future data.

Regularization: Penalizing hypotheses that are more complex to favor, simpler more general hypotheses.
    
    cost(h) = loss(h) + complexity(h) 

Holdout Cross-Validation: Splitting data into a training set and a test set, such that learning happens on the training set and is evaluated on the test set.

k-Fold Cross-Validation: Spillting data into k sets, and expermenting k times, using each set asa a test set once, and using remaining data as training set.

Reinforcement Learning: Given a set of rewards or punishments, learn what actions to take in the future.

Markov Decision Process: Model for decision-making, representing states, actions, and their awards.
    -Set of states S
    -Set of actions ACTIONS(s)
    -Transition model P('s | s, a)
    -Reward function R(s, a, s')

Q-Learning: Method for learning a function Q(s, a), estimate of the value of performing action a in state s.
    -Start with Q(s, a) = 0 for all s, A
    -When we taken an action and receive a reward:
    -Every time we take an action a in state s and observe a reward r, we update:
        -Estimate the value of Q(s, a) based on current reward and expected future reward.
        -Update Q(s, a) to take into account old estimate as well as our new estimate.

Greedy Decision-Making:
    -When in state s, choose action a with highest Q(s, a)

Greedy:
    -Set equal to how often we want to move randomly.
    -With, probability 1, choose estimated best move.
    -With probability, choose a random move.

Function Approximation: Approximation Q(s, a), often by a function combining various features, rather than storing one value for every state-action pair.

Unsupervised Learning: Given input data without any addiational feedbacl, learn patterns.

Clustering: Organizing a set of obhects into gruops in such a way that similar objects tend to be in the same gruop.

Some Clustering Applications:
    
    -Genetic resarch
    -Image segmentation
    -Market resarch
    -Medical imaging
    -Social network analysis

Means Clustering: Algorithm for clustering data based on repeatedly assigning points to clusters and updating those clusters' centers.

Learning: 

    -Supervised Learning
    -Reinforcement Learning
    -Unsupervised Learning